Classifying NK Networks.
In this experiment, we will classify NK networks according to its topology and underlying rules.
Helper Functions.
In[1]:= toBinaryInt[n_,bits_]:=IntegerDigits[n,2,bits]
The following code contains snips from and code inspired by https://demonstrations.wolfram.com/BooleanNKNetworks/
In[2]:= ConnectivityMatrix[n_,k_]:=Table[RandomSample[Flatten[Prepend[Table[0,{n-k}],Table[1,{k}]]]],{n}]
In[3]:= InitialStates[n_]:=RandomInteger[1,n]
In[4]:= PositionsMatrix[conM_]:=Map[Flatten,Position[#,1]&/@conM];
In[5]:= evolve[state_, topo_]:=
Table[
With[
{
inputs = (topo["Connections"])[[i]],
trans = (topo["Rules"])[[i]]
},
trans[state[[inputs]]]
]
,{i,Length@state}]
In[6]:= flattenEvo[ str_,net_,t_]:= Flatten@Rest@NestList[evolve[#,net]&,str,t] 
First, we will define a subset of Boolean functions (the Interesting ones).
Lists of rules.
In[7]:= binToBool[x_]:=
If[x==0,False,True]
In[8]:= boolToBin[x_]:=
If[x,1,0]
Unary
In[9]:= rNot[{x_}]:= boolToBin@Not@binToBool@x
In[10]:= rId[{x_}]:=x
Binary
In[11]:= rAnd[{x1_,x2_}]:= 
boolToBin[(binToBool@x1)&&(binToBool@x2)]
In[12]:= rOr[{x1_,x2_}]:= 
boolToBin[(binToBool@x1)||(binToBool@x2)]
In[13]:= rNand[{x1_,x2_}]:= 
boolToBin[(binToBool@x1)⊼(binToBool@x2)]
In[14]:= rXor[{x1_,x2_}]:= 
boolToBin[(binToBool@x1)⊻(binToBool@x2)]
Tertiary
In[15]:= RuleK3A[{x1_,x2_,x3_}]:=
If[x1+x2+x3==3,1,0]
In[16]:= RuleK3B[{x1_,x2_,x3_}]:=
If[x1+x2+x3==3,0,1]
In[17]:= RuleK3C[{x1_,x2_,x3_}]:=
If[x1+x2+x3==2,1,0]
In[18]:= RuleK3D[{x1_,x2_,x3_}]:=
If[x1+x2+x3==2,0,1]
In[19]:= RuleK3E[{x1_,x2_,x3_}]:=
If[x1+x2+x3==1,1,0]
In[20]:= RuleK3F[{x1_,x2_,x3_}]:=
If[x1+x2+x3==1,0,1]
In[21]:= RuleK3G[{x1_,x2_,x3_}]:=
If[x1+x2+x3==0,1,0]
In[22]:= RuleK3H[{x1_,x2_,x3_}]:=
If[x1+x2+x3==0,0,1]
Function to randomly select rules.
In[23]:= randomRule[k_]:=
Which[
k==1,RandomChoice[{rNot,rId}],
k==2,RandomChoice[{rAnd,rOr,rNand,rXor}], 
(*  k2,RandomChoice[{rNand,rXor}],*)
k==3,RandomChoice[{RuleK3A,RuleK3B,RuleK3C,RuleK3D,RuleK3E,RuleK3F,RuleK3G,RuleK3H}]
]
Function that generates a random NK network topology.
In[24]:= randomNetwork[n_,k_]:=
With[
{
rules = RandomChoice[
randomRule[k]&/@Range[n]
,n],
connections = PositionsMatrix[ConnectivityMatrix[n,k]]
},
Association[
{
"Connections"->connections,
"Rules"-> rules
}
]
]
Function that generates a random NK network topology with reduced number of rules.
In[25]:= (* R is the number of rules on each network*)
randomNetwork[n_,k_,r_]:= 
With[
{
rules = RandomChoice[
randomRule[k]&/@Range[r]
,n],
connections = PositionsMatrix[ConnectivityMatrix[n,k]]
},
Association[
{
"Connections"->connections,
"Rules"-> rules
}
]
]
In[26]:= randomNetwork[10,2,4]
Out[26]= <|Connections->{{1,4},{5,10},{1,6},{7,10},{4,5},{1,9},{4,8},{8,9},{4,8},{8,10}},Rules->{rNand,rAnd,rAnd,rNand,rNand,rNand,rOr,rOr,rNand,rOr}|>
The Sets.
In[27]:= nSamples = 20
Out[27]= 20
In[28]:= nClasses = 10
Out[28]= 10
In[29]:= FactorInteger[nClasses]
Out[29]= {{2,1},{5,1}}
Network parameters.
In[30]:= n = 4;
k = 2;
(* r = 4; *)
t =10;
ini = ConstantArray[0,n];
We will choose 10 classes for each cases: topologies and rules.
I will define a random rule function so that at least one NAND is present and the evolution can always scape from the cero case.
In[34]:= randomRuleL[2,n_] := RandomSample[Flatten@{rNand, randomRule[2]&/@Range[n-1]} ]
In[35]:= SeedRandom[4122019]
classesTopo = DeleteDuplicates[#["Connections"]&/@(randomNetwork[n,k]&/@Range[nClasses])]
classesRules = randomRuleL[k,n] &/@Range[nClasses]
Out[36]= {{{1,3},{2,3},{1,3},{3,4}},{{3,4},{1,3},{2,3},{1,4}},{{1,4},{1,4},{1,2},{2,3}},{{2,3},{3,4},{2,3},{2,4}},{{2,3},{1,4},{2,3},{3,4}},{{1,2},{3,4},{2,3},{2,4}},{{1,3},{2,3},{1,3},{1,2}},{{1,2},{2,3},{2,4},{2,4}},{{2,4},{3,4},{1,3},{2,3}},{{2,3},{2,4},{2,3},{1,4}}}
Out[37]= {{rOr,rOr,rNand,rOr},{rAnd,rOr,rNand,rAnd},{rAnd,rNand,rOr,rAnd},{rNand,rAnd,rAnd,rNand},{rNand,rNand,rNand,rAnd},{rNand,rNand,rOr,rAnd},{rXor,rNand,rXor,rXor},{rOr,rOr,rAnd,rNand},{rXor,rXor,rXor,rNand},{rAnd,rNand,rAnd,rXor}}
Topology Test Set.
In[38]:= SeedRandom[4122019]

TrainingSetTopo = Flatten@Table[
With[
{tp = classesTopo[[j]]},
Table[
With[
{cn = randomRuleL[k,n] },
flattenEvo[ ini,
<|"Connections"-> tp,"Rules"-> cn|>,
t]->j
]
,{i,nSamples}]
]
,{j,Length@classesTopo}
];

ValidationSetTopo = Flatten@Table[
With[
{tp = classesTopo[[j]]},
Table[
With[
{cn = randomRuleL[k,n] },
flattenEvo[ ini,
<|"Connections"-> tp,"Rules"-> cn|>,
t]->j
]
,{i,nSamples}]
]
,{j,Length@classesTopo}
];

TestSetTopo = Flatten@Table[
With[
{tp = classesTopo[[j]]},
Table[
With[
{cn = randomRuleL[k,n] },
flattenEvo[ ini,
<|"Connections"-> tp,"Rules"-> cn|>,
t]->j
]
,{i,10*nSamples}]
]
,{j,Length@classesTopo}
];
In[42]:= Length@TrainingSetTopo
Out[42]= 200
Rules Test Sets
In[43]:= SeedRandom[4122019]

TrainingSetRules = Flatten@Table[
With[
{rl = classesRules[[j]]},
Table[
With[
{cn = randomNetwork[n,k] },
flattenEvo[ ini,
<|"Connections"-> cn["Connections"],"Rules"-> rl|>,
t]->j
]
,{i,nSamples}]
]
,{j,Length@classesTopo}
];

ValidationSetRules = Flatten@Table[
With[
{rl = classesRules[[j]]},
Table[
With[
{cn = randomNetwork[n,k] },
flattenEvo[ ini,
<|"Connections"-> cn["Connections"],"Rules"-> rl|>,
t]->j
]
,{i,nSamples}]
]
,{j,Length@classesTopo}
];

TestSetRules = Flatten@Table[
With[
{rl = classesRules[[j]]},
Table[
With[
{cn = randomNetwork[n,k] },
flattenEvo[ ini,
<|"Connections"-> cn["Connections"],"Rules"-> rl|>,
t]->j
]
,{i,10*nSamples}]
]
,{j,Length@classesTopo}
];
Classifying the Sets with Deep Learning
Let's see how Mathematica does to classify this problem.
Topology:
In[47]:= c1Top = Classify[TrainingSetTopo]
Out[47]= ClassifierFunction[Input type: BooleanVector (length: 40)
Number of classes: 10

]
In[48]:= ClassifierMeasurements[c1Top, TestSetTopo, "Accuracy"]
Out[48]= 0.2075
0.201% is very close to random.
Rules:
In[49]:= c1Rules = Classify[TrainingSetRules]
Out[49]= ClassifierFunction[Input type: BooleanVector (length: 40)
Number of classes: 10

]
In[50]:= ClassifierMeasurements[c1Rules, TestSetRules, "Accuracy"]
Out[50]= 0.8235
0.82 is very good. Better than expected.
A simple neural network.
In[51]:= n*t
Out[51]= 40
In[52]:= Length@TrainingSetRules[[1,1]]
Out[52]= 40
In[53]:= nnT = NetChain[
{ 
LinearLayer[n*t],Ramp,DropoutLayer[],
LinearLayer[],Ramp,SoftmaxLayer[]},
"Output"->NetDecoder[{"Class",Range[Length@classesTopo ]}],"Input"-> {n*t}
]
Out[53]= NetChain[Input port:	vector ( size: 40 )
Output port:	class
Number of layers:	6

]
In[54]:= netT = NetTrain[nnT,
TrainingSetTopo, 
ValidationSet->ValidationSetTopo
,TargetDevice->{"GPU",2} 
]
Out[54]= NetChain[Input port:	vector ( size: 40 )
Output port:	class
Number of layers:	6

]
In[55]:= ClassifierMeasurements[netT, TestSetTopo, "Accuracy"]
Out[55]= 0.3275
Better than nearest neighbor classifier but still low.
For completeness sake, lets classify the rules.
In[56]:= nnR = NetChain[
{ 
LinearLayer[n*t],Ramp,DropoutLayer[],
LinearLayer[],Ramp,SoftmaxLayer[]},
"Output"->NetDecoder[{"Class",Range[Length@classesRules]}],"Input"-> {n*t}
]
Out[56]= NetChain[Input port:	vector ( size: 40 )
Output port:	class
Number of layers:	6

]
In[57]:= netR = NetTrain[nnR,
TrainingSetRules, 
ValidationSet->ValidationSetRules
,TargetDevice->{"GPU",2} 
]
Out[57]= NetChain[Input port:	vector ( size: 40 )
Output port:	class
Number of layers:	6

]
In[58]:= ClassifierMeasurements[netR, TrainingSetRules, "Accuracy"]
Out[58]= 0.925
This is now expected given the previous result.
Algorithmic Information Classifier
Lets compute the conditional CTM for the reduced set of rules. 
In[59]:= set = toBinaryInt[#,8]&/@Range[2^8];
Function that enumerates all the possible topologies.
In[60]:= ConnectivityMatrix[4,2]
Out[60]= {{0,1,1,0},{1,1,0,0},{1,1,0,0},{1,1,0,0}}
In[61]:= conBits [n_,k_] :=Flatten[Prepend[Table[0,{n-k}],Table[1,{k}]]]
In[62]:= rows[n_,k_]:=Permutations[conBits[n,k]]
In[63]:= topo[n_,k_]:=Tuples[rows[n,k],n]
For the following experiment I will focus on networks of size 5.
In[66]:= Length@topo[5,2]
Out[66]= 100000
Now, the list of all networks.
In[67]:= ruleL[k_]:=
Which[
k==1,{rNot,rId},
k==2,{rAnd,rOr,rNand,rXor},
k==3,{RuleK3A,RuleK3B,RuleK3C,RuleK3D,RuleK3E,RuleK3F,RuleK3G,RuleK3H}
]
In[68]:= rulesL[n_,k_]:=Tuples[ruleL[k],n]
In[69]:= Remove[networks]
In[70]:= (* R is the number of rules on each network*)
networks[n_,k_]:= 
(<|"Connections"->PositionsMatrix@#[[1]],"Rules"->#[[2]]|>)&/@With[
{
tps = topo[n,k],
rls = rulesL[n,k]
},
Tuples[{tps,rls}]
]
In[71]:= Length@networks[4,2]
Out[71]= 331776
In[72]:= net =RandomChoice@networks[4,2]
Out[72]= <|Connections->{{1,4},{2,3},{1,2},{1,2}},Rules->{rOr,rNand,rAnd,rAnd}|>
In[73]:= init = ConstantArray[0,4]
Out[73]= {0,0,0,0}
In[74]:= net["Connections"]
Out[74]= {{1,4},{2,3},{1,2},{1,2}}
In[75]:= n2 =randomNetwork[4,2]
Out[75]= <|Connections->{{2,3},{2,4},{1,4},{1,4}},Rules->{rXor,rAnd,rXor,rXor}|>
In[76]:= flattenEvo[init,n2,10]
Out[76]= {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}
In[77]:= flattenEvo[init,net,10]
Out[77]= {0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0}
Now, to compute the incidences.
We will always start with the strings of zeros.
In[78]:= init = ConstantArray[0,n];
In[79]:= nets = networks[n,k];
A topological relation  (t->s) exists of the t produces the string s.
In[80]:=  topologies = Merge[#,Total]&@Table[
With[
{
n= nets[[i]]
},
{n["Connections"],flattenEvo[init,n,t]}->1
]
,
{i,Length@nets}];
In[81]:= SetDirectory[NotebookDirectory[]];
In[82]:= Export["topoN4K2.m",topologies]
Out[82]= topoN4K2.m
In[83]:= Length@(Keys@topologies)
Out[83]= 152148
Let' s see an example of rules.
In[84]:= (Keys@topologies)[[1111]]->(Values@topologies)[[1111]]
Out[84]= {{{1,2},{1,2},{1,3},{3,4}},{1,0,0,0,1,1,1,0,0,1,0,0,1,1,0,0,0,1,1,0,1,1,1,0,0,1,0,0,1,1,0,0,0,1,1,0,1,1,1,0}}->1
In[85]:= rules= Merge[#,Total]&@Table[
With[
{
n= nets[[i]]
},
{n["Rules"],flattenEvo[init,n,t]}->1
]
,
{i,Length@nets}];
In[86]:= Export["rulesN4K2.m",rules]
Out[86]= rulesN4K2.m
In[87]:= Length@(Keys@rules)
Out[87]= 39592
Let's see an example of rules.
In[88]:= (Keys@rules)[[1111]]->(Values@rules)[[1111]]
Out[88]= {{rNand,rXor,rXor,rAnd},{1,0,0,0,1,1,0,0,0,0,1,0,1,0,1,0,1,1,1,1,0,0,0,1,1,0,0,0,1,1,0,0,0,0,1,0,1,0,1,0}}->1
Algorithmic Probability
The following function will be a convenient way to consult the previous database.
In[89]:= fRelTop[s1_,s2_]:=With[
{out = topologies[{s1,s2}]},
If[MissingQ@out,
0,
out
]
]
In[90]:= fRelRul[s1_,s2_]:=With[
{out = rules[{s1,s2}]},
If[MissingQ@out,
0,
out
]
]
For example.
In[91]:= fRelTop[
{{1,2},{1,2},{1,3},{3,4}},
{1,0,0,0,1,1,1,0,0,1,0,0,1,1,0,0,0,1,1,0,1,1,1,0,0,1,0,0,1,1,0,0,0,1,1,0,1,1,1,0}
]
Out[91]= 1
In[92]:= fRelTop[
{{1,2},{1,2},{1,3},{3,4}},
{1,1,0,0,1,1,1,0,0,1,0,0,1,1,0,0,0,1,1,0,1,1,1,0,0,1,0,0,1,1,0,0,0,1,1,0,1,1,1,0}
]
Out[92]= 0
Lets compute the totals.
In[93]:= totalTop = Total@Values@topologies
Out[93]= 331776
In[94]:= totalrules = Total@Values@rules
Out[94]= 331776
In[95]:= Length@nets
Out[95]= 331776
And now the probabilities.
In[96]:= mTop[top_,st_]:=
fRelTop[top,st]/totalTop
In[97]:= mRul[rl_,st_]:=
fRelRul[rl,st]/totalrules
For example:
In[98]:= mTop[
{{1,2},{1,2},{1,3},{3,4}},
{1,0,0,0,1,1,1,0,0,1,0,0,1,1,0,0,0,1,1,0,1,1,1,0,0,1,0,0,1,1,0,0,0,1,1,0,1,1,1,0}
]
Out[98]= 1/331776
Let's define a penalty value for the probability cero cases.
In[99]:= N@(-Log[2,1/totalTop])
Out[99]= 18.3399
In[100]:= penalty = 20
Out[100]= 20
Conditional CTM
In[101]:= kmTopCTM[top_,st_]:=
With[{p = mTop[top,st]},
If[
p<=0,penalty,
 -Log[2,p]
]
]
In[102]:= kmRulCTM[top_,st_]:=
With[{p = mRul[top,st]},
If[
p<=0,penalty,
 -Log[2,p]
]
]
For example.
In[103]:= N@kmTopCTM[
{{1,2},{1,2},{1,3},{3,4}},
{1,0,0,0,1,1,1,0,0,1,0,0,1,1,0,0,0,1,1,0,1,1,1,0,0,1,0,0,1,1,0,0,0,1,1,0,1,1,1,0}
]
Out[103]= 18.3399
The Algorithmic Classifier
Let us now define the model.
The model is composed of either the topology or the rules of each set.
In[104]:= topoSpace = PositionsMatrix/@topo[n,k];
In[105]:= modelTopo1 = AssociationMap[{}&,Range[Length@classesTopo]]
Out[105]= <|1->{},2->{},3->{},4->{},5->{},6->{},7->{},8->{},9->{},10->{}|>
In[106]:= ChooseDat[data_, dig_]:=Select[data, (#[[2]]==dig)&]
The Cost Function
In[107]:= costTop[c_,set_ ]:=
N@Total@Table[
kmTopCTM[c,set[[i]][[1]]]
,{i,Length@set}]
Sanity checks.
In[108]:= costTop[classesTopo[[1]],ChooseDat[TrainingSetTopo, 1]]
Out[108]= 346.192
In[109]:= costTop[classesTopo[[1]],ChooseDat[TrainingSetTopo, 2]]
Out[109]= 385.435
In[110]:= costTop[classesTopo[[3]],ChooseDat[TrainingSetTopo,3]]
Out[110]= 360.797
Now, to train the model.
In[111]:= modelTopo1=Association@Table[
With[
{
set = ChooseDat[TrainingSetTopo,i]
},
i->MinimalBy[topoSpace, costTop[#,set]&][[1]]
]
,{i,Length@classesTopo}
]
Out[111]= <|1->{{1,3},{2,3},{1,3},{3,4}},2->{{3,4},{1,3},{2,3},{1,4}},3->{{1,4},{1,4},{1,2},{2,3}},4->{{2,3},{3,4},{2,3},{2,4}},5->{{2,3},{1,4},{2,3},{3,4}},6->{{1,2},{3,4},{2,3},{2,4}},7->{{1,3},{2,3},{1,3},{1,2}},8->{{1,2},{2,3},{2,4},{2,4}},9->{{2,4},{3,4},{1,3},{2,3}},10->{{2,3},{2,4},{2,3},{1,4}}|>
In[112]:= classesTopo
Out[112]= {{{1,3},{2,3},{1,3},{3,4}},{{3,4},{1,3},{2,3},{1,4}},{{1,4},{1,4},{1,2},{2,3}},{{2,3},{3,4},{2,3},{2,4}},{{2,3},{1,4},{2,3},{3,4}},{{1,2},{3,4},{2,3},{2,4}},{{1,3},{2,3},{1,3},{1,2}},{{1,2},{2,3},{2,4},{2,4}},{{2,4},{3,4},{1,3},{2,3}},{{2,3},{2,4},{2,3},{1,4}}}
Looks like the training predicted perfectly the classes.
Lets see its accuracy.
In[113]:= predTopo[m_,x_]:= MinimalBy[Keys@m, (N@kmTopCTM[m[#],x])& ][[1]]
For example.
In[114]:= predTopo[modelTopo1, (Last@TestSetTopo)[[1]]]
Out[114]= 1
In[115]:= right = 0;
wrong = List[]
Table[
If[
predTopo[
modelTopo1, TestSetTopo[[i]][[1]]
]== TestSetTopo[[i]][[2]],
right = right +1,
wrong=Append[wrong,TestSetTopo[[i]]]
]
,{i,Length[TestSetTopo]}];
Out[116]= {}
In[118]:= right
Out[118]= 1448
And the accuracy is :
In[119]:= N@right/Length[TestSetTopo]
Out[119]= 0.724
0.73 % is a significant improvement.
And our mistakes are in:
In[120]:= Tally@Values@wrong
Out[120]= {{1,10},{2,26},{3,27},{4,39},{5,69},{6,110},{7,78},{8,72},{9,40},{10,81}}
Mostly in the class 6.
Let's do a bit of research on the possible cause of this:
In[121]:= MaximalBy[Tally[Keys@TestSetTopo],#[[2]]&,5]
Out[121]= {{{1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0},116},{{0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1},91},{{0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0},51},{{0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0},47},{{0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1},34}}
So, the string 
In[122]:= s=MaximalBy[Tally[Keys@TestSetTopo],#[[2]]&,5][[1,1]]
Out[122]= {1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0}
Is the most common. Let's see to which classes it belongs too.
In[123]:= rep=Select[TestSetTopo, (#[[1]]==s)&];
In[124]:= Tally@Values@rep
Out[124]= {{1,6},{2,1},{3,4},{4,29},{5,8},{6,21},{7,3},{8,28},{9,7},{10,9}}
As we can see, it belongs to several classes therefore is impossible to correctly classify every time. However, is most probably it belongs to the class 1. Lets see what our classifier tell us:
In[125]:= predTopo[modelTopo1, s]
Out[125]= 4
Yes, as expected, is assigned to class 1!
For completeness sake, lets classify the operations too.
In[126]:= modelRule = AssociationMap[{}&,Range[Length@classesRules]]
Out[126]= <|1->{},2->{},3->{},4->{},5->{},6->{},7->{},8->{},9->{},10->{}|>
In[127]:= costRule[c_,set_ ]:=
N@Total@Table[
kmRulCTM[c,set[[i]][[1]]]
,{i,Length@set}]
Sanity check.
In[128]:= costRule[classesRules[[1]],ChooseDat[TrainingSetRules, 1]]
Out[128]= 264.642
In[129]:= costRule[classesRules[[1]],ChooseDat[TrainingSetRules, 2]]
Out[129]= 276.009
Search space.
In[130]:= rulesSpace =rulesL[n,k];
Now to train the model.
In[131]:= modelRule=Association@Table[
With[
{
set = ChooseDat[TrainingSetRules,i]
},
i->MinimalBy[rulesSpace, costRule[#,set]&][[1]]
]
,{i,Length@classesRules}
]
Out[131]= <|1->{rOr,rOr,rNand,rOr},2->{rAnd,rOr,rNand,rAnd},3->{rAnd,rNand,rOr,rAnd},4->{rNand,rAnd,rAnd,rNand},5->{rNand,rNand,rNand,rAnd},6->{rNand,rNand,rOr,rAnd},7->{rXor,rNand,rXor,rXor},8->{rOr,rOr,rAnd,rNand},9->{rXor,rXor,rXor,rNand},10->{rAnd,rNand,rAnd,rXor}|>
In[132]:= classesRules
Out[132]= {{rOr,rOr,rNand,rOr},{rAnd,rOr,rNand,rAnd},{rAnd,rNand,rOr,rAnd},{rNand,rAnd,rAnd,rNand},{rNand,rNand,rNand,rAnd},{rNand,rNand,rOr,rAnd},{rXor,rNand,rXor,rXor},{rOr,rOr,rAnd,rNand},{rXor,rXor,rXor,rNand},{rAnd,rNand,rAnd,rXor}}
The classifier is:
In[133]:= predRule[m_,x_]:= MinimalBy[Keys@m, (N@kmRulCTM[m[#],x])& ][[1]]
We reconstructed the rules.
Now lets compute the accuracy.
In[134]:= right = 0;
wrong = List[]
Table[
If[
predRule[
modelRule, TestSetRules[[i]][[1]]
]== TestSetRules[[i]][[2]],
right = right +1,
wrong=Append[wrong,TestSetRules[[i]]]
]
,{i,Length[TestSetRules]}];
Out[135]= {}
In[137]:= right
Out[137]= 1827
In[138]:= N@right/Length[TestSetRules]
Out[138]= 0.9135
91.35% accuracy.