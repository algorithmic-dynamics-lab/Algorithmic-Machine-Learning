#!/usr/bin/env wolframscript
(* ::Package:: *)

(* ::Title:: *)
(*Classifying ECA by Their Initial State.*)


(* ::Text:: *)
(*In this experiment, we will try to associate each automaton evolution string to its initialization. *)


(* ::Subchapter:: *)
(*The Sets*)


(* ::Text:: *)
(*First, we get a random number of initialization of strings, with higher probability according to its BDM. Also, Let's keep it small for now.*)


(* ::Text:: *)
(*We will use an accuracy resolution of 16-bits for the inputs and 14-bits for the outputs.*)


toBinaryInt[n_,bits_]:=IntegerDigits[n,2,bits]


set = Range[2^12];


(* ::Text:: *)
(*Computing the weights set using the square of the BDM of the binary representations of the strings.*)


SeedRandom[3012019]
class = RandomSample[set,10]
classB = toBinaryInt[#,12]&/@class


(* ::Subsection:: *)
(*Now, the Data Sets.*)


(* ::Text:: *)
(*Let's use 20 samples per class. *)


nSamples = 20;
size = 4;


SeedRandom[0103019]
TrainingSample = 
DeleteDuplicates@Catenate@Table[
Table[
With[
{aut =RandomInteger[128]},
(Rest@CellularAutomaton[aut,classB[[i]],size])->class[[i]]
]
,{j,nSamples}]
,{i,Length[class]}];

ValidationSample = 
DeleteDuplicates@Catenate@Table[
Table[
With[
{aut =RandomInteger[128]},
(Rest@CellularAutomaton[aut,classB[[i]],size])->class[[i]]
]
,{j,nSamples}]
,{i,Length[class]}];

TestSample = 
DeleteDuplicates@Catenate@Table[
Table[
With[
{aut =RandomInteger[128]},
(Rest@CellularAutomaton[aut,classB[[i]],size])->class[[i]]
]
,{j,nSamples}]
,{i,Length[class]}];



(* ::Text:: *)
(*The data sets are composed of elements of the form:*)


(* ::Input:: *)
(*ex=RandomChoice@TrainingSample*)


(* ::Text:: *)
(*Which means that the matrix:*)


(* ::Input:: *)
(*ArrayPlot@ex[[1]]*)


(* ::Text:: *)
(*Was generated by the string *)


(* ::Input:: *)
(*ArrayPlot@toBinaryInt[{ex[[2]]},16]*)


(* ::Text:: *)
(*For a randomly selected automaton.*)


(* ::Chapter:: *)
(*Solving the Classification Problem With NN.*)


(* ::Text:: *)
(*Let's see if we can solve this problem with NN.*)


(* ::Input:: *)
(*TrainingSample2 = ({#[[1]]}->#[[2]])&/@TrainingSample;*)
(*ValidationSample2 = ({#[[1]]}->#[[2]])&/@ValidationSample;*)
(*TestSample2 = ({#[[1]]}->#[[2]])&/@TestSample;*)


(* ::Input:: *)
(*TrainingSample2[[1]]*)


(* ::Input:: *)
(*nn =  NetChain[{ ConvolutionLayer[12,{3,2}],Ramp,PoolingLayer[{2,9}],FlattenLayer[], LinearLayer[Length@class],SoftmaxLayer[]},*)
(*"Output"->NetDecoder[{"Class",class}],"Input"-> {1,4,12}];*)


(* ::Input:: *)
(*net = NetTrain[nn,TrainingSample2,ValidationSet->ValidationSample2, TargetDevice->{"GPU",2}]*)


(* ::Input:: *)
(*ClassifierMeasurements[net, TestSample2, "Accuracy"]*)


(* ::Input:: *)
(*ClassifierMeasurements[net, TrainingSample2, "Accuracy"]*)


(* ::Text:: *)
(*Classification is barely above random choice.*)


(* ::Text:: *)
(*Let's see if a more general model performs better.*)


(* ::Input:: *)
(*nn2 = NetChain[*)
(*{FlattenLayer[], *)
(*LinearLayer[48],Ramp,DropoutLayer[],*)
(*LinearLayer[],Ramp,*)
(*SoftmaxLayer[]},*)
(*"Output"->NetDecoder[{"Class",class}],"Input"-> {4,12}*)
(*]*)


(* ::Input:: *)
(*SeedRandom[03012019]*)
(*net2 = NetTrain[nn2,TrainingSample,ValidationSet->ValidationSample, TargetDevice->{"GPU",2}]*)


(* ::Input:: *)
(*ClassifierMeasurements[net2, TestSample, "Accuracy"]*)


(* ::Input:: *)
(*ClassifierMeasurements[net2, TrainingSample, "Accuracy"]*)


(* ::Text:: *)
(*It indeed does much better, but 60% is still a low value. I'm actually surprised that the network did this good, though.*)


(* ::Text:: *)
(*Let's see if a deeper model performs better.*)


(* ::Input:: *)
(*nn3 = NetChain[*)
(*{FlattenLayer[], *)
(*LinearLayer[48],Ramp,DropoutLayer[],*)
(*LinearLayer[48],Ramp,DropoutLayer[],*)
(*LinearLayer[],Ramp,SoftmaxLayer[]},*)
(*"Output"->NetDecoder[{"Class",class}],"Input"-> {4,12}*)
(*]*)


(* ::Input:: *)
(*net3 = NetTrain[nn3,TrainingSample,ValidationSet->ValidationSample, TargetDevice->{"GPU",2}]*)


(* ::Input:: *)
(*ClassifierMeasurements[net3, TestSample, "Accuracy"]*)


(* ::Input:: *)
(*ClassifierMeasurements[net3, TrainingSample, "Accuracy"]*)


(* ::Text:: *)
(*It performed worse. So, what if we dramatically increase the depth of the network and give it ample time to optimize it?*)


(* ::Input:: *)
(*nn4 = NetChain[*)
(*{*)
(*FlattenLayer[], LinearLayer[48],Ramp,DropoutLayer[],*)
(*LinearLayer[48],Ramp,DropoutLayer[],*)
(*LinearLayer[48],Ramp,DropoutLayer[],*)
(*LinearLayer[48],Ramp,DropoutLayer[],*)
(*LinearLayer[48],Ramp,DropoutLayer[],*)
(*LinearLayer[48],Ramp,DropoutLayer[],*)
(*LinearLayer[],SoftmaxLayer[]},*)
(*"Output"->NetDecoder[{"Class",class}],"Input"-> {4,12}*)
(*]*)


(* ::Input:: *)
(*net4 = NetTrain[nn4,TrainingSample,ValidationSet->ValidationSample, TargetDevice->{"GPU",2}, *)
(*MaxTrainingRounds->10000]*)


(* ::Input:: *)
(*ClassifierMeasurements[net4, TestSample, "Accuracy"]*)


(* ::Input:: *)
(*ClassifierMeasurements[net4, TrainingSample, "Accuracy"]*)


(* ::Text:: *)
(*Even with dropout layers, we found a plateau.*)


(* ::Chapter:: *)
(*Solving the Problem With an Algorithmic Classifier *)


(* ::Subchapter:: *)
(*The Tools.*)


(* ::Subsubsection:: *)
(*Relationships :*)


(* ::Text:: *)
(*We will say that a relationship s1->s2 exists for the automaton A if the string s2 is an output of the automaton A with input s1.*)


(* ::Section:: *)
(*Conditional ECA CTM.*)


(* ::Text:: *)
(*Given two strings s1 and s2, K(s2|s1) is defined as the amount of information required to construct or define s2 from s1. In formal terms, K(s2|s1) is the smallest program that that produces s2 with s1 as input. Following the coding theorem method, in algorithmic probability theory this value is related to the probability that a randomly chosen Turing Machine will produce s2 given s1 as an input. *)


(* ::Text:: *)
(*Therefore, we will do the following: We will compute a database of all 12-bits strings and their 12x4 outputs for all cellular automatons, partitioned in pairs of 4 and 4x4, 6 and 6x4, and 12 and 12x4. This is done to avoid frontier issues that are present in the way Mathematica computes CA.*)


(* ::Input:: *)
(*SetDirectory[NotebookDirectory[]];*)


(* ::Text:: *)
(*The following object contains the precomputed relationships of all pairs of 6-bit strings within all ECA initialized with 16 bits. Then, we will count how many times each pair is associated.*)


(* ::Input:: *)
(*dist =<< "dataB6-4v2.m";*)


(* ::Input:: *)
(*Length@Keys@dist*)


(* ::Text:: *)
(*The data has the form:*)


(* ::Input:: *)
(*(Keys@dist)[[1]]->(Values@dist)[[1]]*)


(* ::Text:: *)
(*The following function will be a convenient way to consult the previous database. *)


(* ::Input:: *)
(*fRel[s1_,s2_]:=With[*)
(*{out = dist[{s1,s2}]},*)
(*If[MissingQ@out,*)
(*0,*)
(*out*)
(*]*)
(*]*)


(* ::Subsection:: *)
(*Algorithmic Probability*)


(* ::Text:: *)
(*We will define the algorithmic probability of two strings as the probability that, for a randomly chosen ECA, an string containing s1 will output and string containing s2 at the same position.*)


(* ::Input:: *)
(*Length@Values@dist*)


(* ::Input:: *)
(*total = Total@Values@dist*)


(* ::Input:: *)
(*m[s1_,s2_]:=*)
(*fRel[s1,s2]/total*)


(* ::Text:: *)
(*For example:*)


(* ::Input:: *)
(*CellularAutomaton[110,{1,0,1,1,1,0,1,0,1,1,1,0},4]*)


(* ::Input:: *)
(*m[{1,0,1,1,1,0},{{1,1,1,0,1,1},{0,0,1,1,1,0},{0,1,1,0,1,0},{1,1,1,1,1,0}}]*)


(* ::Text:: *)
(*After changing a single bit:*)


(* ::Input:: *)
(*m[{1,0,1,1,1,1},{{1,1,1,0,1,1},{0,0,1,1,1,0},{0,1,1,0,1,0},{1,1,1,1,1,0}}]*)


(* ::Text:: *)
(*Thus, the ECA CTM is defined as:*)


(* ::Text:: *)
(*When the probability of obtaining one string from the other, we will assign a penalty value of 21, which is greater than -Log[2,1/total].*)


(* ::Input:: *)
(*N@(-Log[2,1/total])*)


(* ::Input:: *)
(*penalty = 21;*)


(* ::Input:: *)
(*caCTM[s1_,s2_]:=*)
(*With[{p = m[s1,s2]},*)
(*If[*)
(*p<=0,penalty,*)
(* -Log[2,p]*)
(*]*)
(*]*)


(* ::Input:: *)
(*N@caCTM[{1,0,1,1,1,0},{{1,1,1,0,1,1},{0,0,1,1,1,0},{0,1,1,0,1,0},{1,1,1,1,1,0}}]*)


(* ::Input:: *)
(*caCTM[{1,0,1,1,1,1},{{1,1,1,0,1,1},{0,0,1,1,1,0},{0,1,1,0,1,0},{1,1,1,1,1,0}}]*)


(* ::Section:: *)
(*ECA Conditional BDM*)


(* ::Text:: *)
(*Let us now define the conditional ECABDM of two strings.*)


(* ::Input:: *)
(*Pairs[l1_,l2_]:=Table[*)
(*{l1[[i]],l2[[i]]}*)
(*,{i,Length@l1}]*)


(* ::Input:: *)
(*condBDM[s1_,s2_, part_:6,disp_:6]:=*)
(*N@Total@With[*)
(*{adj1= Partition[s1,part,disp],*)
(*adj2= Flatten[Partition[s2,{4,part},{4,part}],1]*)
(*},*)
(*With[*)
(*{prs = Pairs[adj1,adj2]},*)
(*caCTM[#[[1]],#[[2]]]&/@prs *)
(*]*)
(*]*)


(* ::Input:: *)
(*condBDM[{0,0,0,0,0,0,0,0,0,0,0,0},{{1,1,1,0,1,1,1,1,1,0,1,1},{0,0,1,1,1,0,0,0,1,1,1,0},{0,1,1,0,1,0,0,1,1,0,1,0},{1,1,1,1,1,0,1,1,1,1,1,0}},6,6]*)


(* ::Input:: *)
(*CellularAutomaton[110,{1,0,0,0,1,0,0,0,0,0,0,1},4]*)


(* ::Input:: *)
(*condBDM[{1,0,0,0,1,0,0,0,0,0,0,1},*)
(*{{1,0,0,1,1,0,0,0,0,0,1,1},{1,0,1,1,1,0,0,0,0,1,1,0},{1,1,1,0,1,0,0,0,1,1,1,1},{0,0,1,1,1,0,0,1,1,0,0,0}},6,6]*)


(* ::Subchapter:: *)
(*The Model*)


(* ::Text:: *)
(*For this problem, the model for each class is going to be a 12-bit string.*)


(* ::Input:: *)
(*model1 = AssociationMap[toBinaryInt[0,12]&,class]*)


(* ::Subsection:: *)
(*Training the Model*)


(* ::Text:: *)
(*Let' s start by defining a the cost function(s).*)


(* ::Input:: *)
(*ChooseDat[data_, dig_]:=Select[data, (#[[2]]==dig)&]*)


(* ::Input:: *)
(*cost[center_, set_, dim_:6, disp_:6]:=*)
(*Total@Table[*)
(*condBDM[center,set[[i,1]],dim,disp]^2*)
(*,{i,Length@set}]*)


(* ::Input:: *)
(*TrainingSample[[1]]*)


(* ::Text:: *)
(*For example:*)


(* ::Input:: *)
(*cost[classB[[1]],ChooseDat[TrainingSample,class[[1]]], 6,6]*)


(* ::Input:: *)
(*cost[classB[[1]],ChooseDat[TrainingSample,class[[10]]], 6,6]*)


(* ::Input:: *)
(*cost[{0,0,0,0,0,0,0,0,0,0,0,0},ChooseDat[TrainingSample,class[[10]]], 6,6]*)


(* ::Input:: *)
(* searchSet = toBinaryInt[#,12]&/@Range[0,2^12-1]; *)


(* ::Text:: *)
(*Let's try to train the model by brute force.*)


(* ::Input:: *)
(*Table[*)
(*With[*)
(*{*)
(*ss = ChooseDat[TrainingSample,class[[i]]],*)
(*cls = class[[i]]*)
(*},*)
(*model1[cls]=MinimalBy[searchSet, cost[#,ss,6,6]&,1][[1]]*)
(*]*)
(*,{i,Length@model1}];*)


(* ::Input:: *)
(*model1*)


(* ::Input:: *)
(*Tally@Values@model1*)


(* ::Subchapter:: *)
(*Model Accuracy*)


(* ::Text:: *)
(*Lets test the accuracy of the model.*)


(* ::Input:: *)
(*pred[m_,x_,dim_:6,disp_:6]:= MinimalBy[Keys@m, condBDM[m[#],x,dim,disp]&][[1]]*)


(* ::Input:: *)
(*pred[model1,TrainingSample[[1,1]]]*)


(* ::Input:: *)
(*right = 0;*)
(*wrong = List[]*)
(*Table[*)
(*If[*)
(*pred[*)
(*model1, TestSample[[i]][[1]]*)
(*]== TestSample[[i]][[2]],*)
(*right = right +1,*)
(*wrong=Append[wrong,TestSample[[i]]]*)
(*]*)
(*,{i,Length[TestSample]}];*)


(* ::Text:: *)
(*And the accuracy is:*)


(* ::Input:: *)
(*right*)


(* ::Input:: *)
(*N@right/Length[TestSample]*)


(* ::Input:: *)
(*Length@TestSample*)


(* ::Input:: *)
(*178*)


(* ::Text:: *)
(*Furthermore, the wrong samples were:*)


(* ::Input:: *)
(*Tally@Values@wrong*)


(* ::Text:: *)
(*Which were assigned to the samples:*)


(* ::Input:: *)
(*Tally@(pred[model1, #[[1]],6,6]&/@wrong)*)


(* ::Text:: *)
(*The performance is much better than the neural network classifiers.*)


(* ::Text:: *)
(*Let' s see if we increasing the 'penalty; can improve the accuracy. *)


(* ::Input:: *)
(*penalty = 100;*)


(* ::Input:: *)
(*condBDM[{0,0,0,0,0,0,0,0,0,0,0,0},{{1,1,1,0,1,1,1,1,1,0,1,1},{0,0,1,1,1,0,0,0,1,1,1,0},{0,1,1,0,1,0,0,1,1,0,1,0},{1,1,1,1,1,0,1,1,1,1,1,0}},6,6]*)


(* ::Input:: *)
(*condBDM[{1,0,0,0,1,0,0,0,0,0,0,1},*)
(*{{1,0,0,1,1,0,0,0,0,0,1,1},{1,0,1,1,1,0,0,0,0,1,1,0},{1,1,1,0,1,0,0,0,1,1,1,1},{0,0,1,1,1,0,0,1,1,0,0,0}},6,6]*)


(* ::Input:: *)
(*Table[*)
(*With[*)
(*{*)
(*ss = ChooseDat[TrainingSample,class[[i]]],*)
(*cls = class[[i]]*)
(*},*)
(*model1[cls]=MinimalBy[searchSet, cost[#,ss,6,6]&,1][[1]]*)
(*]*)
(*,{i,Length@model1}];*)


(* ::Text:: *)
(*And the accuracy:*)


(* ::Input:: *)
(*right = 0;*)
(*wrong = List[]*)
(*Table[*)
(*If[*)
(*pred[*)
(*model1, TestSample[[i]][[1]],6,6*)
(*]== TestSample[[i]][[2]],*)
(*right = right +1,*)
(*wrong=Append[wrong,TestSample[[i]]]*)
(*]*)
(*,{i,Length[TestSample]}];*)


(* ::Input:: *)
(*N@right/Length[TestSample]*)


(* ::Text:: *)
(*Performance is identical, therefore the choice of 21-bits for the penalty was right.*)


(* ::Subchapter:: *)
(*The Perfect Model*)


(* ::Text:: *)
(*Now, lets see the model that the algorithm chose*)


(* ::Input:: *)
(*Values@model1 == classB*)


(* ::Text:: *)
(*The model chose the binary strings corresponding to the sample generators!*)


(* ::Text:: *)
(*This means the model itself, if we can call it like that, is nearly perfect. However, is the prediction which is determining the accuracy of the model itself. I believe that this an important point of difference with the traditional AI models, given that tuning the prediction function is the model in itself. *)


(* ::Text:: *)
(*So, how lets see how the perfect classifier would perform. This is the one who, for each  input, searches directly for the input string.*)


(* ::Text:: *)
(*The following function will tell us if there exists an automaton that produces x given the input.*)


(* ::Input:: *)
(*existA[in_,x_]:=Count[Table[*)
(*CellularAutomaton[i,in,4][[2;;5]]==x*)
(*,{i,0,128}*)
(*],*)
(*True*)
(*]*)


(* ::Input:: *)
(*indx = AssociationThread[classB,class];*)


(* ::Input:: *)
(*perfPred[x_]:=*)
(*indx[MaximalBy[classB,existA[#,x]&][[1]]]*)


(* ::Input:: *)
(*perfPred[x]*)


(* ::Input:: *)
(*right = 0;*)
(*wrongP = List[]*)
(*Table[*)
(*If[*)
(*perfPred[*)
(*TestSample[[i]][[1]]*)
(*]== TestSample[[i]][[2]],*)
(*right = right +1,*)
(*wrongP=Append[wrongP,TestSample[[i]]]*)
(*]*)
(*,{i,Length[TestSample]}];*)


(* ::Input:: *)
(*right*)


(* ::Input:: *)
(*N@right/Length[TestSample]*)


(* ::Input:: *)
(*right = 0;*)
(*wrongP = List[]*)
(*Table[*)
(*If[*)
(*perfPred[*)
(*TrainingSample[[i]][[1]]*)
(*]== TestSample[[i]][[2]],*)
(*right = right +1,*)
(*wrongP=Append[wrongP,TestSample[[i]]]*)
(*]*)
(*,{i,Length[TestSample]}];*)


(* ::Text:: *)
(*As we can see from this, the algorithmic classifier with partition size of 12 is already as good as we can expect it to be, which is an expected result.*)


(* ::Subchapter:: *)
(*An Smarter Training Method.*)


(* ::Text:: *)
(*Will do greedy approaches on training the previous model.*)


(* ::Input:: *)
(*Remove[ model2, model3, train, fullTrain]*)


(* ::Section:: *)
(*First Algorithm.*)


(* ::Text:: *)
(*The first algorithm will, first, train the first 6-bits and then the second.*)


(* ::Text:: *)
(*Let's reset the model.*)


(* ::Input:: *)
(*model2 = AssociationMap[toBinaryInt[0,12]&,class]*)


(* ::Text:: *)
(*The following function will find the best sub-vector of size 4 that minimizes the cost function on each specific position.*)


(* ::Input:: *)
(*searchSet=toBinaryInt[#,6]&/@Range[2^6];*)


(* ::Text:: *)
(*Useful for the training function.*)


(* ::Input:: *)
(*(* Replaces a part of a vector. *)*)
(*replace[vect_,new_,st_]:=*)
(*If[*)
(*(st-1) < Length@vect,*)
(*Catenate[*)
(*{vect[[1;;st]]*)
(*,new,*)
(*vect[[st+(Length@new)+1;;All]]}*)
(*] ,*)
(*Catenate[*)
(*{vect[[1;;st]],new}*)
(*]*)
(*]*)


(* ::Text:: *)
(*The training function. We will iterate over this function.*)


(* ::Input:: *)
(*train[st_,model_,searchSet_:searchSet]:=*)
(*AssociationThread[*)
(*Keys@model ->Table[*)
(*With[*)
(*{*)
(*cls = class[[i]],*)
(*old = model[class[[i]]],*)
(*ss = ChooseDat[TrainingSample,class[[i]]]*)
(*},*)
(*With[*)
(*{r = MinimalBy[*)
(*searchSet, *)
(*cost[*)
(*replace[old,#,st],*)
(*ss,*)
(*6,6*)
(*]*)
(*&,1][[1]] },*)
(*replace[old,r,st]*)
(*]*)
(*]*)
(*,{i, Length@class}*)
(*]*)
(*]*)


(* ::Text:: *)
(*Now, we train this model.*)


(* ::Input:: *)
(*intervals = {0,6};*)


(* ::Input:: *)
(*fullTrain[model_,interval_, searchSet_:searchSet]:=*)
(*Fold[*)
(*train[#2,#1,searchSet]&, model,interval*)
(*] *)


(* ::Input:: *)
(*model2 = AssociationMap[toBinaryInt[0,12]&,class];*)


(* ::Input:: *)
(*model2 = fullTrain[model2, intervals, searchSet]*)


(* ::Text:: *)
(*Let's measure the accuracy of the model.*)


(* ::Input:: *)
(*right = 0;*)
(*wrong2 = List[]*)
(*Table[*)
(*If[*)
(*pred[*)
(*model2, TestSample[[i]][[1]],6,6*)
(*]== TestSample[[i]][[2]],*)
(*right = right +1,*)
(*wrong2=Append[wrong2,TestSample[[i]]]*)
(*]*)
(*,{i,Length[TestSample]}];*)


(* ::Input:: *)
(*N@right/Length[TestSample]*)


(* ::Text:: *)
(*The performance remains high.*)


(* ::Text:: *)
(*Now on the test sample.*)


(* ::Input:: *)
(*right = 0;*)
(*wrong3 = List[]*)
(*Table[*)
(*If[*)
(*pred[*)
(*model2, TrainingSample[[i]][[1]],6,6*)
(*]== TrainingSample[[i]][[2]],*)
(*right = right +1,*)
(*wrong2=Append[wrong2,TrainingSample[[i]]]*)
(*]*)
(*,{i,Length[TrainingSample]}];*)


(* ::Input:: *)
(*N@right/Length[TrainingSample]*)


(* ::Subsubsection:: *)
(*Other Methods.*)


(* ::Text:: *)
(*First, let us try a different searching order.*)


(* ::Input:: *)
(*model3 = AssociationMap[toBinaryInt[0,12]&,class]*)


(* ::Input:: *)
(*intervals2 = {6,0};*)


(* ::Input:: *)
(*model3 =  fullTrain[model3, intervals2, searchSet]*)


(* ::Input:: *)
(*Values@model3==Values@model2*)


(* ::Input:: *)
(*Table[*)
(*(Values@model3)[[i]]==(Values@model2)[[i]]*)
(*,{i,Length@model3}]*)


(* ::Input:: *)
(*right = 0;*)
(*wrong3 = List[]*)
(*Table[*)
(*If[*)
(*pred[*)
(*model3, TestSample[[i]][[1]],6,6*)
(*]== TestSample[[i]][[2]],*)
(*right = right +1,*)
(*wrong3=Append[wrong3,TestSample[[i]]]*)
(*]*)
(*,{i,Length[TestSample]}];*)


(* ::Input:: *)
(*N@right/Length[TestSample]*)


(* ::Text:: *)
(*The performance is the same,?*)


(* ::Subsection:: *)
(*Another Option.*)


(* ::Text:: *)
(*Another method would be to decrease the size of the training block with blocks of size 12 being effectively exhaustive search.*)


(* ::Subsubsection:: *)
(*For example:*)


(* ::Input:: *)
(*searchSetS8 =toBinaryInt[#,8]&/@Range[2^8];*)


(* ::Input:: *)
(*searchSetS4 =toBinaryInt[#,4]&/@Range[2^4];*)


(* ::Input:: *)
(*model4 = AssociationMap[toBinaryInt[0,12]&,class]*)


(* ::Text:: *)
(*First, we will search for a bigger segment of 8-bits.*)


(* ::Input:: *)
(*model4 =train[0,model4,searchSetS8]*)


(* ::Text:: *)
(*Now, we add the last 4 bits.*)


(* ::Input:: *)
(*model4 =train[8,model4,searchSetS4]*)


(* ::Text:: *)
(*Let us test the accuracy of this model.*)


(* ::Input:: *)
(*right = 0;*)
(*wrong4 = List[]*)
(*Table[*)
(*If[*)
(*pred[*)
(*model4, TestSample[[i]][[1]],6,6*)
(*]== TestSample[[i]][[2]],*)
(*right = right +1,*)
(*wrong4=Append[wrong4,TestSample[[i]]]*)
(*]*)
(*,{i,Length[TestSample]}];*)


(* ::Input:: *)
(*N@right/Length[TestSample]*)


(* ::Text:: *)
(*Performance unexpectedly dropped compared to 6-bit strings.*)


(* ::Text:: *)
(*Let's try 10-bit strings.*)


(* ::Input:: *)
(*model5 = AssociationMap[toBinaryInt[0,12]&,class]*)


(* ::Text:: *)
(*First, we will search for a bigger segment of 10 - bits.*)


(* ::Input:: *)
(*searchSetS10 =toBinaryInt[#,10]&/@Range[2^10];*)


(* ::Input:: *)
(*searchSetS2 =toBinaryInt[#,2]&/@Range[2^2];*)


(* ::Input:: *)
(*model5 =train[0,model5,searchSetS10]*)


(* ::Text:: *)
(*Then, the last 2-bits.*)


(* ::Input:: *)
(*model5 =train[10,model5,searchSetS2]*)


(* ::Input:: *)
(*right = 0;*)
(*wrong5 = List[]*)
(*Table[*)
(*If[*)
(*pred[*)
(*model5, TestSample[[i]][[1]],6,6*)
(*]== TestSample[[i]][[2]],*)
(*right = right +1,*)
(*wrong5=Append[wrong5,TestSample[[i]]]*)
(*]*)
(*,{i,Length[TestSample]}];*)


(* ::Input:: *)
(*N@right/Length[TestSample]*)


(* ::Text:: *)
(*Better than size 8/4 but worse than 6/6. The reason is that Conditional ECA BDM uses partition of size 6.*)


(* ::Subchapter:: *)
(*Generalization of Algorithmic Learning.*)


(* ::Text:: *)
(*Let us try a harder problem. We will now classify problems of size 24x4.*)


(* ::Input:: *)
(*set24 = Range[2^24];*)


(* ::Input:: *)
(*nClasses = 20;*)


(* ::Input:: *)
(*class= RandomSample[set24,nClasses]*)
(*classB= toBinaryInt[#,24]&/@class*)


(* ::Input:: *)
(*SeedRandom[2803019]*)
(*TrainingSample24 = *)
(*DeleteDuplicates@Catenate@Table[*)
(*Table[*)
(*With[*)
(*{aut =RandomInteger[128]},*)
(*(Rest@CellularAutomaton[aut,classB[[i]],size])->class[[i]]*)
(*]*)
(*,{j,nSamples}]*)
(*,{i,Length[class]}];*)
(**)
(*ValidationSample24 = *)
(*DeleteDuplicates@Catenate@Table[*)
(*Table[*)
(*With[*)
(*{aut =RandomInteger[128]},*)
(*(Rest@CellularAutomaton[aut,classB[[i]],size])->class[[i]]*)
(*]*)
(*,{j,nSamples}]*)
(*,{i,Length[class]}];*)
(**)
(*TestSample24 = *)
(*DeleteDuplicates@Catenate@Table[*)
(*Table[*)
(*With[*)
(*{aut =RandomInteger[128]},*)
(*(Rest@CellularAutomaton[aut,classB[[i]],size])->class[[i]]*)
(*]*)
(*,{j,nSamples}]*)
(*,{i,Length[class]}];*)


(* ::Text:: *)
(*We will now train the best performing neural network on this data.*)


(* ::Input:: *)
(*nn5 = NetChain[*)
(*{FlattenLayer[], *)
(*LinearLayer[24*4],Ramp,DropoutLayer[],*)
(*LinearLayer[24*4],Ramp,DropoutLayer[],*)
(*LinearLayer[],Ramp,SoftmaxLayer[]},*)
(*"Output"->NetDecoder[{"Class",class}],"Input"-> {4,24}*)
(*]*)


(* ::Input:: *)
(*SeedRandom[31416]*)
(*net5 = NetTrain[nn5,TrainingSample24,ValidationSet->ValidationSample24, TargetDevice->{"GPU",2}]*)


(* ::Input:: *)
(*ClassifierMeasurements[net5, TestSample24, "Accuracy"]*)


(* ::Text:: *)
(*Now, lets try the smaller model.*)


(* ::Input:: *)
(*nn6 = NetChain[*)
(*{FlattenLayer[], *)
(*LinearLayer[24*4],Ramp,DropoutLayer[],*)
(*LinearLayer[],Ramp,SoftmaxLayer[]},*)
(*"Output"->NetDecoder[{"Class",class}],"Input"-> {4,24}*)
(*]*)


(* ::Input:: *)
(*SeedRandom[31416]*)
(*net6 = NetTrain[nn6,TrainingSample24,ValidationSet->ValidationSample24, TargetDevice->{"GPU",2}]*)


(* ::Input:: *)
(*ClassifierMeasurements[net6, TestSample24, "Accuracy"]*)


(* ::Text:: *)
(*We can say that the performance is lower.*)


(* ::Subsection:: *)
(*The Performance of an Algorithmic Classifier.*)


(* ::Input:: *)
(*model24 = AssociationMap[toBinaryInt[0,24]&,class];*)


(* ::Input:: *)
(*train[st_,model_,searchSet_:searchSet, data_]:=*)
(*AssociationThread[*)
(*Keys@model ->Table[*)
(*With[*)
(*{*)
(*cls = class[[i]],*)
(*old = model[class[[i]]],*)
(*ss = ChooseDat[data,class[[i]]]*)
(*},*)
(*With[*)
(*{r = MinimalBy[*)
(*searchSet, *)
(*cost[*)
(*replace[old,#,st],*)
(*ss,*)
(*6,6*)
(*]*)
(*&,1][[1]] },*)
(*replace[old,r,st]*)
(*]*)
(*]*)
(*,{i, Length@class}*)
(*]*)
(*]*)


(* ::Input:: *)
(*class*)


(* ::Text:: *)
(*Sanity check.*)


(* ::Input:: *)
(*cost[classB[[1]],ChooseDat[TrainingSample24,class[[1]]], 6,6]*)


(* ::Input:: *)
(*cost[classB[[12]],ChooseDat[TrainingSample24,class[[1]]], 6,6]*)


(* ::Input:: *)
(*x = classB[[2]]*)


(* ::Input:: *)
(*model24 =train[0,model24,searchSet, TrainingSample24];*)


(* ::Input:: *)
(*model24 =train[6,model24,searchSet, TrainingSample24];*)


(* ::Input:: *)
(*model24 =train[12,model24,searchSet, TrainingSample24];*)


(* ::Input:: *)
(*model24 =train[18,model24,searchSet, TrainingSample24];*)


(* ::Text:: *)
(*Lets measure the accuracy of the model. *)


(* ::Input:: *)
(*acc[model_, set_]:=*)
(*Total@Table[*)
(*If[*)
(*pred[*)
(*model, set[[i]][[1]],6,6*)
(*]== set[[i]][[2]],*)
(*1,*)
(*0*)
(*]*)
(*,{i,Length[set]}]/(Length@set);*)
(**)


(* ::Input:: *)
(*N@acc[model24,TestSample24]*)


(* ::Text:: *)
(*In contrast, the accuracy of the algorithmic model increased to 96%!*)


(* ::Input:: *)
(*right = 0;*)
(*wrong3 = List[]*)
(*Table[*)
(*If[*)
(*pred[*)
(*model2, TrainingSample24[[i]][[1]],6,6*)
(*]== TrainingSample24[[i]][[2]],*)
(*right = right +1,*)
(*wrong2=Append[wrong2,TrainingSample24[[i]]]*)
(*]*)
(*,{i,Length[TrainingSample24]}];*)


(* ::Input:: *)
(*N@acc[model24,TrainingSample24]*)
